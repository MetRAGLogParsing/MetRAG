{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Core\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Transformers / Unsloth\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# LangChain pieces you actually use\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "gdcUjMim9Rzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Preprocessing**"
      ],
      "metadata": {
        "id": "sjkokv2QlOrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Context Dataset and Test Dataset\n",
        "\n",
        "TestDoc = pd.read_csv(\"Context.csv\") #Context dataset\n",
        "TestLogs = pd.read_csv(\"TestLogs.csv\") #Test dataset\n",
        "TestLogs_Cols = TestLogs[['Content','EventTemplate','Source','Category']]\n",
        "\n",
        "documents = [\n",
        "    Document(page_content=f\"Log: {row['Content']}\\nTemplate: {row['EventTemplate']}\")\n",
        "    for _, row in TestDoc.iterrows()\n",
        "]\n"
      ],
      "metadata": {
        "id": "enM72sHAARnv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Rule Dataset\n",
        "\n",
        "rules_df = pd.read_csv(\"rules.csv\")\n",
        "rules_dict = {\n",
        "    row['RuleNumber']: {\n",
        "        'rule': row['Rule'],\n",
        "        'example': row['Example'],\n",
        "        'template': row['Template']\n",
        "    }\n",
        "    for _, row in rules_df.iterrows()\n",
        "}"
      ],
      "metadata": {
        "id": "iKhkpDdXgkzg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunking\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "wbVuV_8fA2U1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Embedding Model\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# Embedding Context Data Chunks\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "e3x04TnFDAAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Retrieval**"
      ],
      "metadata": {
        "id": "CUalovlfGR8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity-based Retrieval\n",
        "\n",
        "def similarity_retrieval(query, threshold=0.7, k=3):\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    results = vectorstore.similarity_search_by_vector(query_embedding, k=k)\n",
        "\n",
        "    filtered_results = []\n",
        "    similarity_scores = []\n",
        "    for doc in results:\n",
        "        doc_embedding = embeddings.embed_query(doc.page_content)\n",
        "        sim = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
        "        if sim >= threshold:\n",
        "            filtered_results.append(doc.page_content)\n",
        "            similarity_scores.append(sim)\n",
        "\n",
        "    similarity_context_str = \"\\n\".join(filtered_results) if filtered_results else \"\"\n",
        "    similarity_score = float(np.mean(similarity_scores)) if similarity_scores else 0.0\n",
        "\n",
        "    return similarity_context_str, similarity_score, len(filtered_results) > 0"
      ],
      "metadata": {
        "id": "KwonlmvnLKl1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule-matching\n",
        "\n",
        "def match_with_priority(text):\n",
        "    matched_spans = []\n",
        "    results = {\n",
        "        'rule1': [],\n",
        "        'rule2': [],\n",
        "        'rule3': [],\n",
        "        'rule4': [],\n",
        "        'rule5': [],\n",
        "        'rule6': [],\n",
        "        'rule7': [],\n",
        "        'rule8': [],\n",
        "        'rule9': [],\n",
        "        'rule10': []\n",
        "    }\n",
        "\n",
        "    # Rules regex matching ,Priority-ordered patterns with weights\n",
        "    patterns = {\n",
        "        'rule1': (r'/(?:\\d{1,3}\\.){3}\\d{1,3}:\\d+\\b', 0.9),  # IP with Port\n",
        "        'rule2': (r'(/[^\\s]+(?:\\.[^\\s]+)?)', 0.7),         # File Path\n",
        "        'rule3': (r'\\b(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}\\b', 0.9),  # MAC address\n",
        "        'rule4': (r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', 0.8),   # IP address\n",
        "        'rule5': (r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}:\\d+\\b|\\b(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}:\\d+\\b', 0.8),  #Domain:Port\n",
        "        'rule6': (r'\\b0x[0-9A-Fa-f]+\\b', 0.6),            # Hexadecimal Token\n",
        "        'rule7': (r'\\b[\\w]+(:[\\w]+){2,}\\b', 0.6),         # Timestamp\n",
        "        'rule8': (r'(?<![\\d:.])\\b\\d+\\b(?![:.])', 0.5),    # Numeric Identifier\n",
        "        'rule9': (r'\\b(?=[\\w.-]*[A-Za-z])(?=[\\w.-]*\\d)[\\w.-]+\\b', 0.5),  # Alphanumeric Token\n",
        "        'rule10': (r'\\b[A-Za-z ]+\\b', 0.3),               # Static Message\n",
        "    }\n",
        "\n",
        "    def is_overlapping(start, end):\n",
        "        for s, e in matched_spans:\n",
        "            if start < e and end > s:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    for rule_name, (pattern, _) in patterns.items():\n",
        "        for match in re.finditer(pattern, text):\n",
        "            start, end = match.span()\n",
        "            if not is_overlapping(start, end):\n",
        "                matched_spans.append((start, end))\n",
        "                results[rule_name].append(match.group())\n",
        "\n",
        "    final_output = {}\n",
        "    for rule, matches in results.items():\n",
        "        final_output[rule] = {\n",
        "            'match': 'yes' if matches else 'no',\n",
        "            'patterns': matches\n",
        "        }\n",
        "    return final_output, patterns\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vYrnq_RGvXIF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule-based Retrieval\n",
        "def rule_based_retrieval(query):\n",
        "    rule_matches, patterns = match_with_priority(query)\n",
        "\n",
        "    rule_context = []\n",
        "    rule_score = 0.0\n",
        "    matched_rules = 0\n",
        "\n",
        "    for rule, info in rule_matches.items():\n",
        "        if info['match'] == 'yes':\n",
        "            matched_rules += 1\n",
        "            rule_info = rules_dict[rule]\n",
        "            rule_context.append(\n",
        "                f\"Example: {rule_info['example']}\\n\"\n",
        "                f\"Template: {rule_info['template']}\"\n",
        "            )\n",
        "            rule_score += patterns[rule][1]\n",
        "\n",
        "    rule_context = rule_context[:3]  # keep top-3\n",
        "    rule_context_str = \"\\n\\n\".join(rule_context) if rule_context else \"\"\n",
        "    rule_score = (rule_score / max(1, matched_rules)) if matched_rules > 0 else 0.0\n",
        "\n",
        "    return rule_context_str, rule_score, matched_rules > 0"
      ],
      "metadata": {
        "id": "pBm2spu_K_Q0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric-based Retieval\n",
        "\n",
        "def retrieve_with_threshold(query, rule_weight=0.5, similarity_weight=0.5):\n",
        "    # Rule-based\n",
        "    rule_context, rule_score, rules_matched = rule_based_retrieval(query)\n",
        "\n",
        "    # Similarity-based\n",
        "    similarity_context, similarity_score, similarity_found = similarity_retrieval(query)\n",
        "\n",
        "    # Compare\n",
        "    combined_score_rule = rule_weight * rule_score\n",
        "    combined_score_similarity = similarity_weight * similarity_score\n",
        "\n",
        "    if combined_score_rule > combined_score_similarity and rules_matched:\n",
        "        selected_context = rule_context\n",
        "        context_source = \"Rule-Based\"\n",
        "    elif combined_score_similarity >= combined_score_rule and similarity_found:\n",
        "        selected_context = similarity_context\n",
        "        context_source = \"Similarity-Based\"\n",
        "    else:\n",
        "        selected_context = (\n",
        "            f\"### Rule-Based Context:\\n{rule_context}\\n\\n\"\n",
        "            f\"### Similarity-Based Context:\\n{similarity_context}\"\n",
        "        )\n",
        "        context_source = \"Combined\"\n",
        "\n",
        "    return selected_context, {\n",
        "        \"rule_score\": rule_score,\n",
        "        \"similarity_score\": similarity_score,\n",
        "        \"context_source\": context_source,\n",
        "\n",
        "    }"
      ],
      "metadata": {
        "id": "6P6dtnIARrrI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Generation**"
      ],
      "metadata": {
        "id": "pfQoAsVtmjp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "FastLanguageModel.for_inference(model)\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "3Xj4jbKfjIDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "\n",
        "\"\"\"Inference\"\"\"\n",
        "prompt = \"\"\"You are a log parsing assistant. Your task is to extract the template of the given log message \"\n",
        "            \"by replacing dynamic parts like timestamps, IDs, IP addresses, or numeric values with the '<*>' placeholder. \"\n",
        "            \"Use the examples in the context to guide your output. \"\n",
        "            \"If the log message is fully static and has no dynamic parts, return it as-is with no placeholders. \"\n",
        "            \"Return ONLY the extracted template with no explanations, reasoning, or additional text.\"\n",
        "### Context:\n",
        "{selected_context}\n",
        "\n",
        "### Log Message to Parse:\n",
        "```\n",
        "{log}\n",
        "```\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def extract_log_templates(new_logs, retriever, tokenizer, model, device):\n",
        "    results = []\n",
        "    for idx, row in enumerate(new_logs.itertuples(), start=1):\n",
        "        log = row.Content if isinstance(row.Content, str) else str(row.Content)\n",
        "        category = row.Category\n",
        "        source = row.Source\n",
        "\n",
        "        # retriever returns TWO things\n",
        "        selected_context, scores = retriever(log)\n",
        "\n",
        "        formatted_prompt = prompt.format(\n",
        "            selected_context=selected_context, log=log\n",
        "        ) + EOS_TOKEN\n",
        "\n",
        "        print(f\"\\nProcessing Log {idx}...\")\n",
        "        print(\"=\" * 100)\n",
        "        print(f\"**Log Message:** {log}\")\n",
        "        print(f\"**Context Source:** {scores['context_source']}\")\n",
        "        print(f\"**Scores:** Rule={scores['rule_score']:.2f}, Similarity={scores['similarity_score']:.2f}\")\n",
        "        print(f\"**LLM Input Prompt:**\\n{formatted_prompt}\\n\")\n",
        "\n",
        "        inputs = tokenizer([formatted_prompt], return_tensors='pt').to(device)\n",
        "        start_time = time.time()\n",
        "        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "        llm_response = output_text.split(\"### Response:\")[-1].strip()\n",
        "        extracted_template = llm_response.replace(\"`\", \"\").strip()\n",
        "        extracted_template = (\n",
        "            extracted_template.split(EOS_TOKEN)[0].strip()\n",
        "            if EOS_TOKEN in extracted_template else extracted_template\n",
        "        )\n",
        "        if extracted_template.startswith(\"Template:\"):\n",
        "            extracted_template = extracted_template[len(\"Template:\"):].strip()\n",
        "        elif extracted_template.startswith(\"template:\"):\n",
        "            extracted_template = extracted_template[len(\"template:\"):].strip()\n",
        "\n",
        "        print(f\"**LLM Response:**\\n{llm_response}\\n\")\n",
        "        print(f\"**Extracted Template:** {extracted_template}\")\n",
        "        print(f\"Inference Time: {elapsed_time:.2f} seconds\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        results.append({\n",
        "            \"Log\": log,\n",
        "            \"Extracted Template\": extracted_template,\n",
        "            \"Category\": category,\n",
        "            \"Source\": source,\n",
        "            \"Context_Source\": scores['context_source'],\n",
        "            \"Rule_Score\": scores['rule_score'],\n",
        "            \"Similarity_Score\": scores['similarity_score'],\n",
        "        })\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "oBCyFDMpvFoV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the Pipeline\n",
        "\n",
        "def process_datasets(dataset, retriever, tokenizer, model, device):\n",
        "    results = extract_log_templates(dataset, retriever, tokenizer, model, device)\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"MetRAG_Results.csv\", index=False) # Save Results Dataset\n",
        "    print(\"Extracted templates saved successfully.\")\n",
        "\n",
        "# Run the pipeline\n",
        "start_time = time.time()  # Start timer\n",
        "process_datasets(TestLogs_Cols, retrieve_with_threshold, tokenizer, model, device)\n",
        "end_time = time.time()  # End timer\n",
        "total_inference_time = end_time - start_time # Inference Time\n",
        "print(f\"Total inference time: {total_inference_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "P6NiS1HT4VtQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}